<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19591666</article-id><article-id pub-id-type="pmc">2724423</article-id><article-id pub-id-type="publisher-id">1471-2105-10-213</article-id><article-id pub-id-type="doi">10.1186/1471-2105-10-213</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>A comparison of random forest and its Gini importance with standard chemometric methods for the feature selection and classification of spectral data</article-title></title-group><contrib-group><contrib id="A1" contrib-type="author"><name><surname>Menze</surname><given-names>Bjoern H</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I2">2</xref><email>bjoern.menze@iwr.uni-heidelberg.de</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Kelm</surname><given-names>B Michael</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>michael.kelm@iwr.uni-heidelberg.de</email></contrib><contrib id="A3" contrib-type="author"><name><surname>Masuch</surname><given-names>Ralf</given-names></name><xref ref-type="aff" rid="I3">3</xref><email>masuch@micro-biolytics.com</email></contrib><contrib id="A4" contrib-type="author"><name><surname>Himmelreich</surname><given-names>Uwe</given-names></name><xref ref-type="aff" rid="I4">4</xref><email>uwe.himmelreich@med.kuleuven.be</email></contrib><contrib id="A5" contrib-type="author"><name><surname>Bachert</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="I5">5</xref><email>p.bachert@dkfz-heidelberg.de</email></contrib><contrib id="A6" contrib-type="author"><name><surname>Petrich</surname><given-names>Wolfgang</given-names></name><xref ref-type="aff" rid="I6">6</xref><xref ref-type="aff" rid="I7">7</xref><email>wolfgang.petrich@roche.com</email></contrib><contrib id="A7" corresp="yes" contrib-type="author"><name><surname>Hamprecht</surname><given-names>Fred A</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I6">6</xref><email>fred.hamprecht@iwr.uni-heidelberg.de</email></contrib></contrib-group><aff id="I1"><label>1</label>Interdisciplinary Center for Scientific Computing (IWR), University of Heidelberg, Heidelberg, Germany</aff><aff id="I2"><label>2</label>Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology, Cambridge/MA, USA</aff><aff id="I3"><label>3</label>Micro-Biolytics GmbH, Esslingen, Germany</aff><aff id="I4"><label>4</label>Biomedical NMR Unit, Department of Medical Diagnostic Sciences, KU Leuven, Leuven, Belgium</aff><aff id="I5"><label>5</label>Department of Medical Physics in Radiology, German Cancer Research Center, Heidelberg, Germany</aff><aff id="I6"><label>6</label>Department of Astronomy and Physics, University of Heidelberg, Heidelberg, Germany</aff><aff id="I7"><label>7</label>Roche Diagnostics GmbH, Mannheim, Germany</aff><pub-date pub-type="collection"><year>2009</year></pub-date><pub-date pub-type="epub"><day>10</day><month>7</month><year>2009</year></pub-date><volume>10</volume><fpage>213</fpage><lpage>213</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10/213"/><history><date date-type="received"><day>23</day><month>2</month><year>2009</year></date><date date-type="accepted"><day>10</day><month>7</month><year>2009</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2009 Menze et al; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2009</copyright-year><copyright-holder>Menze et al; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p><!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>
               Menze
               H
               Bjoern
               
               
               bjoern.menze@iwr.uni-heidelberg.de
            </dc:author><dc:title>
            A comparison of random forest and its Gini importance with standard chemometric methods for the feature selection and classification of spectral data
         </dc:title><dc:date>2009</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 10(1): 213-. (2009)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2009)10:1&#x0003c;213&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>--></license></permissions><abstract><sec><title>Background</title><p>Regularized regression methods such as principal component or partial least squares regression perform well in learning tasks on high dimensional spectral data, but cannot explicitly eliminate irrelevant features. The random forest classifier with its associated Gini feature importance, on the other hand, allows for an explicit feature elimination, but may not be optimally adapted to spectral data due to the topology of its constituent classification trees which are based on orthogonal splits in feature space.</p></sec><sec><title>Results</title><p>We propose to combine the best of both approaches, and evaluated the joint use of a feature selection based on a recursive feature elimination using the Gini importance of random forests' together with regularized classification methods on spectral data sets from medical diagnostics, chemotaxonomy, biomedical analytics, food science, and synthetically modified spectral data. Here, a feature selection using the Gini feature importance with a regularized classification by discriminant partial least squares regression performed as well as or better than a filtering according to different univariate statistical tests, or using regression coefficients in a backward feature elimination. It outperformed the direct application of the random forest classifier, or the direct application of the regularized classifiers on the full set of features.</p></sec><sec><title>Conclusion</title><p>The Gini importance of the random forest provided superior means for measuring feature relevance on spectral data, but &#x02013; on an optimal subset of features &#x02013; the regularized classifiers might be preferable over the random forest classifier, in spite of their limitation to model linear dependencies only. A feature selection based on Gini importance, however, may precede a regularized linear classification to identify this optimal subset of features, and to earn a double benefit of both dimensionality reduction and the elimination of noise from the classification task.</p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>The high dimensionality of the feature space is a characteristic of learning problems involving spectral data. In many applications with a biological or biomedical background addressed by, for example, nuclear magnetic resonance or infrared spectroscopy, also the number of available samples <italic>N </italic>is lower than the number of features in the spectral vector <italic>P</italic>. The intrinsic dimensionality <italic>P</italic><sub><italic>intr </italic></sub>of spectral data, however, is often much lower than the nominal dimensionality <italic>P </italic>&#x02013; sometimes even below N.</p><sec><title>Dimension reduction and feature selection in the classification of spectral data</title><p>Most methods popular in chemometrics exploit this relation <italic>P</italic><sub><italic>intr</italic></sub><italic> &#x0003c; P </italic>and aim at regularizing the learning problem by <italic>implicitly </italic>restricting its free dimensionality to <italic>P</italic><sub><italic>intr</italic></sub>.</p><p>(Here, and in the following we will adhere to the algorithmic classification of feature selection approaches from [<xref ref-type="bibr" rid="B1">1</xref>], referring to regularization approaches which explicitly calculate a subset of input features &#x02013; in a preprocessing, for example &#x02013; as <italic>explicit </italic>feature selection methods, and to approaches performing a feature selection or dimension reduction without calculating these subsets as <italic>implicit </italic>feature selection methods.) Popular methods in chemometrics, such as principal component regression (PCR) or partial least squares regression (PLS) directly seek for solutions in a space spanned by ~<italic>P</italic><sub><italic>intr </italic></sub>principal components (PCR) &#x02013; assumed to approximate the intrinsic subspace of the learning problem &#x02013; or by biasing projections of least squares solutions towards this subspace [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B3">3</xref>], down-weighting irrelevant features in a constrained regression (PLS). It is observed, however, that although both PCR and PLS are capable learning methods on spectral data &#x02013; used for example for in [<xref ref-type="bibr" rid="B4">4</xref>] &#x02013; they still have a need to eliminate useless predictors [<xref ref-type="bibr" rid="B5">5</xref>,<xref ref-type="bibr" rid="B6">6</xref>]. Thus, often an additional <italic>explicit </italic>feature selection is pursued in a preceding step to eliminate spectral regions which do not provide any relevant signal at all, showing resonances or absorption bands that can clearly be linked to artefacts, or features which are unrelated to the learning task. Discarding irrelevant feature dimensions, though, raises the question of how to choose such an appropriate subset of features [<xref ref-type="bibr" rid="B6">6</xref>-<xref ref-type="bibr" rid="B8">8</xref>].</p><p>Different univariate and multivariate importance measures can be used to rank features and to select them accordingly [<xref ref-type="bibr" rid="B1">1</xref>]. Univariate tests marginalize over all but one feature and rank them in accordance to their discriminative power [<xref ref-type="bibr" rid="B9">9</xref>,<xref ref-type="bibr" rid="B10">10</xref>]. In contrast, multivariate approaches consider several or all features simultaneously, evaluating the joint distribution of some or all features and estimating their relevance to the overall learning task. Multivariate tests are often used in wrapper schemes in combination with a subsequent classifier (e.g. a global optimization of feature subset and classifier coefficients [<xref ref-type="bibr" rid="B11">11</xref>]), or by statistical tests on the outcome of a learning algorithm (e.g. an iterative regression with test for robustness [<xref ref-type="bibr" rid="B12">12</xref>,<xref ref-type="bibr" rid="B13">13</xref>]). While univariate approaches are sometimes deemed too simplistic, the other group of multivariate feature selection methods often comes at unacceptably high computational costs.</p></sec><sec><title>Gini feature importance</title><p>A feature selection based on the random forest classifier [<xref ref-type="bibr" rid="B14">14</xref>] has been found to provide multivariate feature importance scores which are relatively cheap to obtain, and which have been successfully applied to high dimensional data, arising from microarrays [<xref ref-type="bibr" rid="B15">15</xref>-<xref ref-type="bibr" rid="B20">20</xref>], time series [<xref ref-type="bibr" rid="B21">21</xref>], even on spectra [<xref ref-type="bibr" rid="B22">22</xref>,<xref ref-type="bibr" rid="B23">23</xref>]. Random forest is an ensemble learner based on randomized decision trees (see [<xref ref-type="bibr" rid="B24">24</xref>] for a review of random forests in chemometrics, [<xref ref-type="bibr" rid="B14">14</xref>] for the original publication, and [<xref ref-type="bibr" rid="B25">25</xref>-<xref ref-type="bibr" rid="B28">28</xref>] for methodological aspects), and provides different feature important measures. One measure is motivated from statistical permutation tests, the other is derived from the training of the random forest classifier. Both measures have been found to correlate reasonably well [<xref ref-type="bibr" rid="B28">28</xref>]. While the majority of the prior studies focused on the first, we will focus on the second in the following.</p><p>As a classifier, random forest performs an <italic>implicit </italic>feature selection, using a small subset of "strong variables" for the classification only [<xref ref-type="bibr" rid="B27">27</xref>], leading to its superior performance on high dimensional data. The outcome of this implicit feature selection of the random forest can be visualized by the "Gini importance" [<xref ref-type="bibr" rid="B14">14</xref>], and can be used as a general indicator of feature relevance. This feature importance score provides a relative ranking of the spectral features, and is &#x02013; technically &#x02013; a by-product in the training of the random forest classifier: At each node <italic>&#x003c4; </italic>within the binary trees <italic>T </italic>of the random forest, the optimal split is sought using the Gini impurity <italic>i</italic>(<italic>&#x003c4;</italic>) &#x02013; a computationally efficient approximation to the entropy &#x02013; measuring how well a potential split is separating the samples of the two classes in this particular node.</p><p>With <inline-formula><inline-graphic xlink:href="1471-2105-10-213-i1.gif"/></inline-formula> being the fraction of the <italic>n</italic><sub><italic>k </italic></sub>samples from class <italic>k </italic>= {0,1} out of the total of <italic>n </italic>samples at node <italic>&#x003c4;</italic>, the Gini impurity <italic>i</italic>(<italic>&#x003c4;</italic>) is calculated as</p><p><disp-formula><graphic xlink:href="1471-2105-10-213-i2.gif"/></disp-formula></p><p>Its decrease &#x00394;<italic>i </italic>that results from splitting and sending the samples to two sub-nodes <italic>&#x003c4;</italic><sub><italic>l </italic></sub>and <italic>&#x003c4;</italic><sub><italic>r </italic></sub>(with respective sample fractions <inline-formula><inline-graphic xlink:href="1471-2105-10-213-i3.gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="1471-2105-10-213-i4.gif"/></inline-formula>) by a threshold t<sub><italic>&#x003b8; </italic></sub>on variable <italic>&#x003b8; </italic>is defined as</p><p><disp-formula><graphic xlink:href="1471-2105-10-213-i5.gif"/></disp-formula></p><p>In an exhaustive search over all variables <italic>&#x003b8; </italic>available at the node (a property of the random forest is to restrict this search to a random subset of the available features [<xref ref-type="bibr" rid="B14">14</xref>]), and over all possible thresholds t<sub><italic>&#x003b8;</italic></sub>, the pair {<italic>&#x003b8;</italic>, t<sub><italic>&#x003b8;</italic></sub>} leading to a maximal &#x00394;<italic>i </italic>is determined. The decrease in Gini impurity resulting from this optimal split &#x00394;i<sub><italic>&#x003b8; </italic></sub>(<italic>&#x003c4;</italic>, <italic>T</italic>) is recorded and accumulated for all nodes <italic>&#x003c4; </italic>in all trees <italic>T </italic>in the forest, individually for all variables <italic>&#x003b8;</italic>:</p><p><disp-formula><graphic xlink:href="1471-2105-10-213-i6.gif"/></disp-formula></p><p>This quantity &#x02013; the Gini importance <italic>I</italic><sub><italic>G </italic></sub>&#x02013; finally indicates how often a particular feature <italic>&#x003b8; </italic>was selected for a split, and how large its overall discriminative value was for the classification problem under study.</p><p>When used as an indicator of feature importance for an explicit feature selection in a recursive elimination scheme [<xref ref-type="bibr" rid="B1">1</xref>] and combined with the random forest itself as classifier in the final step, the feature importance measures of the random forest have been found to reduce the amount of features. Most studies using the Gini importance [<xref ref-type="bibr" rid="B22">22</xref>,<xref ref-type="bibr" rid="B29">29</xref>] and the related permutation-based feature importance of random forests [<xref ref-type="bibr" rid="B16">16</xref>,<xref ref-type="bibr" rid="B18">18</xref>,<xref ref-type="bibr" rid="B20">20</xref>,<xref ref-type="bibr" rid="B21">21</xref>,<xref ref-type="bibr" rid="B23">23</xref>] together with random forests in a recursive feature elimination scheme, also showed an increases in prediction performance. (Only [<xref ref-type="bibr" rid="B17">17</xref>] reports a constant performance, but with greatly reduced amount of features.) While these experiments indicate the efficiency of the Gini importance in an explicit <italic>feature selection </italic>[<xref ref-type="bibr" rid="B24">24</xref>] one might raise the question whether a random forest &#x02013; the "native" classifier of Gini importance &#x02013; with its orthogonal splits of feature space is optimal also for the <italic>classification </italic>of spectra with correlated features and data-specific noise (Fig. <xref ref-type="fig" rid="F1">1</xref>), or if other classification models may be a better match with properties of spectral data.</p><fig position="float" id="F1"><label>Figure 1</label><caption><p><bold>Decision trees separating two classes: Classification problem with uncorrelated features (left), and a distorted version resulting from an additive noise process (right)</bold>. The said process induces correlation by adding a random value to both features, thus mimicking the acquisition process of many absorption, reflectance or resonance spectra (see Methods section). Growing orthogonal decision trees on such a data set &#x02013; shown on the right &#x02013; results in deeply nested trees with complex decision boundaries. (Both trees not grown to full depth for visualization purposes).</p></caption><graphic xlink:href="1471-2105-10-213-1"/></fig></sec><sec><title>Objective of this study</title><p>Thus, in the present work, we were interested in evaluating the combination of a feature selection by Gini importance together with standard chemometric classification approaches, such as discriminant PCR and PLS classification (D-PCR and D-PLS, respectively) which are known to be well adapted to spectra, and in studying their performance in dependence of specific characteristics of spectral data. In a first experiment we evaluated the joint application of explicit and implicit dimension reduction, using uni- and multivariate feature selection strategies <italic>in combination </italic>with random forest, D-PLS and D-PCR classification in an explicit recursive feature elimination (Table <xref ref-type="table" rid="T1">1</xref>). In a second experiment, we studied the influence of different noise processes on random forest and D-PLS classification to identify optimal conditions for explicit and implicit dimension reduction. In both experiments we were interested in identifying general properties and differences of the methods employed in the classification of spectral data.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Recursive feature selection.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left"><monospace><bold>1.</bold></monospace></td><td align="left"><monospace><bold>Calculate feature importance </bold>on the training data</monospace></td></tr><tr><td></td><td align="left">&#x02003;<monospace>a. Gini importance</monospace></td></tr><tr><td></td><td align="left">&#x02003;<monospace>b. absolute value of regression coefficients (PLS/PCR)</monospace></td></tr><tr><td></td><td align="left">&#x02003;<monospace>c. p-values from Wilcoxon-test/t-test</monospace></td></tr><tr><td align="left"><monospace><bold>2.</bold></monospace></td><td align="left"><monospace><bold>Rank the features </bold>according to the importance measure, remove the p% least important</monospace></td></tr><tr><td align="left"><monospace><bold>3.</bold></monospace></td><td align="left"><monospace><bold>Train the classifier </bold>on the training data</monospace></td></tr><tr><td></td><td align="left">&#x02003;<monospace>A. Random forest</monospace></td></tr><tr><td></td><td align="left">&#x02003;<monospace>B. D-PLS</monospace></td></tr><tr><td></td><td align="left">&#x02003;<monospace>C. D-PCR</monospace></td></tr><tr><td></td><td align="left"><monospace><bold>and apply </bold>it to the test data</monospace></td></tr><tr><td align="left"><monospace><bold>4.</bold></monospace></td><td align="left"><monospace><bold>Repeat </bold>1.&#x02013;4. until no features are remaining</monospace></td></tr><tr><td align="left"><monospace><bold>5.</bold></monospace></td><td align="left"><monospace><bold>Identify the best </bold>feature subset according to the test error</monospace></td></tr></tbody></table><table-wrap-foot><p>Workflow of the recursive feature selection, and combinations of feature importance measures (1.a-1.c) and classifiers (3.A-3.C) tested in this study. Compare with results in Table 2 and Fig. 4. Hyper-parameters of PLS/PCR/random forest are optimized both in the feature selection (1.) and the classification (3.) step utilizing the training data only. While Gini importance (1.a) and regression coefficients (1.b) have to be calculated within each loop (step 1.&#x02013;4.), the univariate measures (1.c) have only to be calculated once.</p></table-wrap-foot></table-wrap></sec></sec><sec><title>Results and discussion</title><sec><title>Visualizing feature importance</title><p>Measuring feature relevance using the Gini importance is subject to selection bias on factorial data [<xref ref-type="bibr" rid="B30">30</xref>]. Splits are more often sought on variables with a higher number of different factors, and a correction of the Gini importance is necessary in such cases [<xref ref-type="bibr" rid="B30">30</xref>-<xref ref-type="bibr" rid="B32">32</xref>]. Spectral data, except for count data, represent continuous signals, with a distribution of <italic>N </italic>different values for each spectral channel or feature. Each feature will allow the same number of distinct splits in a random forest classification, and, hence, a measurement of the relevance of spectral regions for a specific classification problem will be unaffected by this potential source of bias.</p><p>Both univariate tests for significant class differences returned smooth importance vectors when employed on the spectral data (Fig. <xref ref-type="fig" rid="F2">2</xref>, top). The smoothness of the Gini importance was dependent on the size of the random forest (Fig. <xref ref-type="fig" rid="F2">2</xref>, bottom) &#x02013; small forests resulted in "noisy" importance vectors, only converging towards smooth vectors when increasing the overall number of trees in the forest or the overall number of splits. As such changes influence the absolute value of this measure, the Gini importance could not be interpreted in absolute terms &#x02013; like the p-values of the univariate tests &#x02013; but only allowed for a relative comparison. For such a comparison between different variables and between different measures, the features were ranked according to their importance score (Fig. <xref ref-type="fig" rid="F3">3A</xref>). Here, univariate importance measures and Gini importance agreed well in many, although not all, spectral regions (Fig. <xref ref-type="fig" rid="F3">3A</xref>, rows 2 and 3). An example of the most prominent differences between univariate feature importance and multivariate Gini importance are highlighted in Fig. <xref ref-type="fig" rid="F3">3B</xref>. Spectral regions deemed unimportant by the univariate measures &#x02013; with complete overlap of the marginal distributions as shown in Fig. <xref ref-type="fig" rid="F3">3B</xref> &#x02013; may be attributed high importance by the multivariate importance measure (Fig. <xref ref-type="fig" rid="F4">4</xref>), indicating spectral regions with features of higher order interaction.</p><fig position="float" id="F2"><label>Figure 2</label><caption><p><bold>Importance measures on NMR <italic>candida </italic>data</bold>. in the range from 0.35 to 4 ppm (indicated in the upper figure) for all 1500 spectral channels (indicated in the lower figure). Top: p-values of a t-test (black) and Wilcoxon test (gray). Below: Gini importance of a random forest with 3000 trees (gray) and 6000 trees (black). Compare t ranked measures in Fig. 3.</p></caption><graphic xlink:href="1471-2105-10-213-2"/></fig><fig position="float" id="F3"><label>Figure 3</label><caption><p><bold>Comparison of the different feature selection measures applied to the NMR <italic>candida 2 </italic>data (3A)</bold>. Multivariate feature importance measures can select variables that are discarded by univariate measures (3B). Fig. 3A, from top to bottom: Gini importance, absolute values; Gini importance, ranked values, p-values from t-test, ranked values. Fig. 3B: Feature importance scores below (black: Gini importance, gray: t-test). Perhaps surprisingly, regions with complete overlap of the marginal distributions (3B bottom, indicated by vertical lines), are assigned importance by the multivariate measure (3B top). This is indicative of higher-order interaction effects which can be exploited when used as a feature importance measure with a subsequent classifier.</p></caption><graphic xlink:href="1471-2105-10-213-3"/></fig><fig position="float" id="F4"><label>Figure 4</label><caption><p><bold>Tukey mean-difference plot of univariate and multivariate feature importance (left) and correlation of the importance measures shown in Fig. 3A</bold>. Horizontal lines in the left Fig. indicate differences of more than two sigma, the vertical line in the right Fig. indicates a threshold on the univariate P-value of 0.05 (with relevant features being to the right of the vertical line). -. The importances assigned by univariate and multivariate measures are generally highly correlated; many of the features marked in red (corresponding to the spectral channels indicated in Fig. 3B), however, are flagged as uninformative by a univariate measure and as relevant by a multivariate measure.</p></caption><graphic xlink:href="1471-2105-10-213-4"/></fig><p>Inspecting the Gini feature importance we observed &#x02013; similar to [<xref ref-type="bibr" rid="B32">32</xref>] &#x02013; that some spectral regions were selected as a whole, suggesting that correlated variables were assigned similar importance. Thus, the importance measure may be interpreted like a spectrum, where neighbouring channels of similar importance may be considered as representatives of the same peak, absorbance or resonance line. This can be used in an exploratory visualization of feature relevance (Figs. <xref ref-type="fig" rid="F2">2</xref> and <xref ref-type="fig" rid="F3">3A</xref>, top row). As the random forest prefers splits on correlated variable over splits on uncorrelated ones [<xref ref-type="bibr" rid="B28">28</xref>] it should be noted, however, that this "importance spectrum" may be somewhat biased towards overestimating the importance of major peaks spanning over many spectral channels.</p></sec><sec><title>Feature selection and classification</title><p>The classification accuracies provided by the first experiment based on the real data allowed for a quantitative comparison of the methods applied and for testing for statistically significant differences between results on the full set of features in comparison to the subselected data sets (Table <xref ref-type="table" rid="T2">2</xref>, "stars"). On one half of the data, the feature selection hardly changed the classification performance at all (Tables <xref ref-type="table" rid="T2">2</xref> and <xref ref-type="table" rid="T3">3</xref>, <italic>tumor </italic>and <italic>candida </italic>data), while on the other half a feature selection improved the final result significantly (Tables <xref ref-type="table" rid="T2">2</xref> and <xref ref-type="table" rid="T3">3</xref>, <italic>wine </italic>and <italic>BSE </italic>data), almost independently of the subsequent classifier. In the latter group optimally subselected data typically comprised about 1&#x02013;10% of the initial features (Table <xref ref-type="table" rid="T3">3</xref>, Fig. <xref ref-type="fig" rid="F5">5</xref>). Such a data dependence in the benefit of a preceding feature selection is well known (e.g. [<xref ref-type="bibr" rid="B33">33</xref>]). Different from [<xref ref-type="bibr" rid="B33">33</xref>], however, we did not see a relation to the apparent degree of ill-posedness of the classification problem (i.e., a low ratio <italic>N/P </italic>of the length of the spectral vector <italic>P </italic>and the number of available samples <italic>N </italic>leading to an underdetermined estimation problem &#x02013; he <italic>BSE </italic>and <italic>candida </italic>data, for example, are nearly identical in dimensionality &#x02013; <italic>P</italic><sub><italic>BSE </italic></sub>= <italic>1209</italic>, <italic>P</italic><sub><italic>candida </italic></sub>= <italic>1500 </italic>&#x02013; and number of training samples &#x02013; <italic>N</italic><sub><italic>BSE </italic></sub>= <italic>2 * 96</italic>, <italic>N</italic><sub><italic>candida </italic></sub>= <italic>2 * 101</italic>).</p><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Average cross-validated prediction accuracy.</p></caption><table frame="hsides" rules="groups"><thead><tr><td></td><td></td><td align="center" colspan="3">no selection</td><td align="center" colspan="3">Univariate selection</td><td align="center" colspan="3">Multivariate selection (Gini importance)</td><td align="center" colspan="3">multivariate selection (PLS/PC)</td></tr><tr><td></td><td></td><td colspan="12"><hr></hr></td></tr><tr><td></td><td></td><td align="center">PLS</td><td align="center">PC</td><td align="center">RF</td><td align="center">PLS</td><td align="center">PC</td><td align="center">RF</td><td align="center">PLS</td><td align="center">PC</td><td align="center">RF</td><td align="center">PLS</td><td align="center">PC</td><td align="center">RF</td></tr></thead><tbody><tr><td align="center">MIR BSE</td><td align="center">orig</td><td align="center">66.8</td><td align="center">62.9</td><td align="center">74.9</td><td align="center">80.7</td><td align="center">80.7</td><td align="center">76.7</td><td align="center"><bold><underline>84.1</underline></bold></td><td align="center"><bold>83.2</bold></td><td align="center">77.4</td><td align="center">68</td><td align="center">63.5</td><td align="center">75.5</td></tr><tr><td></td><td></td><td align="center">-</td><td align="center">-</td><td align="center">-</td><td align="center">***</td><td align="center">***</td><td align="center">*</td><td align="center">***</td><td align="center">***</td><td align="center">**</td><td align="center">**</td><td></td><td></td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">binned</td><td align="center">72.7</td><td align="center">73.4</td><td align="center">75.3</td><td align="center">80.4</td><td align="center">80.7</td><td align="center">76.6</td><td align="center"><bold><underline>86.8</underline></bold></td><td align="center"><bold>85.8</bold></td><td align="center">77.3</td><td align="center"><bold>85</bold></td><td align="center">82.1</td><td align="center">75.6</td></tr><tr><td></td><td></td><td align="center">-</td><td align="center">-</td><td align="center">-</td><td align="center">***</td><td align="center">***</td><td align="center">**</td><td align="center">***</td><td align="center">***</td><td align="center">**</td><td align="center">***</td><td align="center">***</td><td></td></tr><tr><td colspan="14"><hr></hr></td></tr><tr><td align="center">MIR wine</td><td align="center">French</td><td align="center">69.5</td><td align="center">69.3</td><td align="center"><bold>79.3</bold></td><td align="center"><bold><underline>83.7</underline></bold></td><td align="center"><bold>83.5</bold></td><td align="center"><bold>82.2</bold></td><td align="center"><bold>82.4</bold></td><td align="center"><bold>81</bold></td><td align="center"><bold>81.2</bold></td><td align="center">66.9</td><td align="center">70.0</td><td align="center"><bold>79.8</bold></td></tr><tr><td></td><td></td><td align="center">-</td><td align="center">-</td><td align="center">-</td><td align="center">***</td><td align="center">**</td><td></td><td align="center">***</td><td align="center">**</td><td align="center">*</td><td></td><td></td><td></td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">grape</td><td align="center">77</td><td align="center">71.4</td><td align="center">90.2</td><td align="center"><bold>98.1</bold></td><td align="center"><bold><underline>98.7</underline></bold></td><td align="center">90.3</td><td align="center"><bold>98.4</bold></td><td align="center"><bold>98.4</bold></td><td align="center">94.2</td><td align="center">91.7</td><td align="center">88.5</td><td align="center">90.4</td></tr><tr><td></td><td></td><td align="center">-</td><td align="center">-</td><td align="center">-</td><td align="center">***</td><td align="center">***</td><td></td><td align="center">***</td><td align="center">***</td><td align="center">**</td><td align="center">***</td><td align="center">***</td><td></td></tr><tr><td colspan="14"><hr></hr></td></tr><tr><td align="center">NMR tumor</td><td align="center">all</td><td align="center">88.8</td><td align="center">89</td><td align="center">89</td><td align="center">89.3</td><td align="center"><bold>89.3</bold></td><td align="center"><bold><underline>90.5</underline></bold></td><td align="center"><bold>90.0</bold></td><td align="center"><bold>89.6</bold></td><td align="center"><bold>89.6</bold></td><td align="center"><bold>89.3</bold></td><td align="center">89.2</td><td align="center">89.1</td></tr><tr><td></td><td></td><td align="center">-</td><td align="center">-</td><td align="center">-</td><td align="center">*</td><td></td><td align="center">***</td><td align="center">**</td><td></td><td align="center">*</td><td></td><td></td><td></td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">center</td><td align="center">71.6</td><td align="center"><bold>72.3</bold></td><td align="center"><bold>73.1</bold></td><td align="center"><bold>73.9</bold></td><td align="center"><bold>72.7</bold></td><td align="center"><bold>73.9</bold></td><td align="center"><bold>72.6</bold></td><td align="center"><bold>72.0</bold></td><td align="center"><bold><underline>74.3</underline></bold></td><td align="center"><bold>71.8</bold></td><td align="center"><bold>72.7</bold></td><td align="center"><bold>73.3</bold></td></tr><tr><td></td><td></td><td align="center">-</td><td align="center">-</td><td align="center">-</td><td align="center">**</td><td></td><td></td><td align="center">*</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="14"><hr></hr></td></tr><tr><td align="center">NMR candida</td><td align="center">1</td><td align="center"><bold>94.9</bold></td><td align="center"><bold>94.6</bold></td><td align="center">90.3</td><td align="center"><bold>95.1</bold></td><td align="center"><bold>94.9</bold></td><td align="center">90.6</td><td align="center"><bold><underline>95.6</underline></bold></td><td align="center"><bold>95.3</bold></td><td align="center">90.3</td><td align="center"><bold>95.3</bold></td><td align="center"><bold>95.2</bold></td><td align="center">90.7</td></tr><tr><td></td><td></td><td align="center">-</td><td align="center">-</td><td align="center">-</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">2</td><td align="center"><bold>95.6</bold></td><td align="center"><bold>95.2</bold></td><td align="center">93.2</td><td align="center"><bold>95.8</bold></td><td align="center"><bold>95.7</bold></td><td align="center">93.7</td><td align="center"><bold>95.6</bold></td><td align="center"><bold>95.5</bold></td><td align="center">93.5</td><td align="center"><bold><underline>96.0</underline></bold></td><td align="center"><bold>95.9</bold></td><td align="center">94.1</td></tr><tr><td></td><td></td><td align="center">-</td><td align="center">-</td><td align="center">-</td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center">*</td><td></td><td></td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">3</td><td align="center">93.7</td><td align="center"><bold>93.8</bold></td><td align="center">89.7</td><td align="center"><bold>93.7</bold></td><td align="center"><bold>93.8</bold></td><td align="center">89.9</td><td align="center"><bold><underline>94.2</underline></bold></td><td align="center"><bold>93.8</bold></td><td align="center">89.9</td><td align="center"><bold>94.0</bold></td><td align="center"><bold>94.0</bold></td><td align="center">90.2</td></tr><tr><td></td><td></td><td align="center">-</td><td align="center">-</td><td align="center">-</td><td></td><td></td><td></td><td align="center"><bold>*</bold></td><td></td><td align="center">*</td><td align="center">*</td><td></td><td></td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">4</td><td align="center">86.9</td><td align="center"><bold>87.3</bold></td><td align="center">83.9</td><td align="center"><bold>87.8</bold></td><td align="center"><bold>87.3</bold></td><td align="center">84.0</td><td align="center"><bold><underline>88.2</underline></bold></td><td align="center"><bold>87.6</bold></td><td align="center">84.3</td><td align="center"><bold>87.7</bold></td><td align="center"><bold>87.6</bold></td><td align="center">84.1</td></tr><tr><td></td><td></td><td align="center">-</td><td align="center">-</td><td align="center">-</td><td></td><td></td><td></td><td align="center"><bold>*</bold></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">5</td><td align="center"><bold>92.7</bold></td><td align="center"><bold>92.6</bold></td><td align="center">89.2</td><td align="center"><bold>92.7</bold></td><td align="center"><bold>92.6</bold></td><td align="center">89.9</td><td align="center"><bold>92.5</bold></td><td align="center"><bold>92.5</bold></td><td align="center">90.3</td><td align="center"><bold><underline>92.8</underline></bold></td><td align="center"><bold>92.6</bold></td><td align="center">90.0</td></tr><tr><td></td><td></td><td align="center">-</td><td align="center">-</td><td align="center">-</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><table-wrap-foot><p>The best classification results on each data set are underlined. Approaches which do not differ significantly from the optimal result (at a 0.05 significance level) are set in bold type (see methods section). Significant differences in the performance of a method as compared to the same classifier without feature selection are marked with asterisks (* p-value &#x0003c; 0.05, ** p-value &#x0003c; 0.01, *** p-value &#x0003c; .001). The MIR data of this table benefit significantly from a feature selection, whereas the NMR data do so only to a minor extent. Overall, a feature selection by means of Gini importance in conjunction with a PLS classifier was successful in all cases and superior to the "native" classifier of Gini importance, the random forest, in all but one cases.</p></table-wrap-foot></table-wrap><fig position="float" id="F5"><label>Figure 5</label><caption><p><bold>Classification accuracy (left column) and standard error (right column) during the course of recursive feature elimination for PLS regression (black), PC regression (dark gray) and random forest (light gray), in combination with different feature selection criteria: univariate (dotted), PLS/PC regression (dashed) and Gini importance (solid)</bold>.</p></caption><graphic xlink:href="1471-2105-10-213-5"/></fig><table-wrap position="float" id="T3"><label>Table 3</label><caption><p>Benefit from feature selection.</p></caption><table frame="hsides" rules="groups"><thead><tr><td></td><td></td><td align="center" colspan="3">univariate selection</td><td align="center" colspan="3">multivariate selection (Gini importance)</td><td align="center" colspan="3">multivariate selection (PLS/PC)</td></tr><tr><td></td><td></td><td colspan="9"><hr></hr></td></tr><tr><td></td><td></td><td align="center">PLS</td><td align="center">PC</td><td align="center">RF</td><td align="center">PLS</td><td align="center">PC</td><td align="center">RF</td><td align="center">PLS</td><td align="center">PC</td><td align="center">RF</td></tr></thead><tbody><tr><td align="center">MIR BSE</td><td align="center">orig</td><td align="center">10.0 (6)</td><td align="center">10.0 (4)</td><td align="center">1.5 (7)</td><td align="center">10.0 (6)</td><td align="center">10.0 (6)</td><td align="center">2.0<break/>(6)</td><td align="center">3.0 (13)</td><td align="center">0.9 (80)</td><td align="center">0.5 (51)</td></tr><tr><td></td><td colspan="10"><hr></hr></td></tr><tr><td></td><td align="center">binned</td><td align="center">6.0<break/>(5)</td><td align="center">7.0<break/>(5)</td><td align="center">2.2 (9)</td><td align="center">10.0 (9)</td><td align="center">10.0 (6)</td><td align="center">3.0<break/>(9)</td><td align="center">10.0 (5)</td><td align="center">9.0<break/>(4)</td><td align="center">0.4 (51)</td></tr><tr><td colspan="11"><hr></hr></td></tr><tr><td align="center">MIR wine</td><td align="center">French</td><td align="center">4.0<break/>(3)</td><td align="center">3.0<break/>(2)</td><td align="center">0.7 (64)</td><td align="center">5.0<break/>(3)</td><td align="center">3.0<break/>(1)</td><td align="center">3.0 (26)</td><td align="center">0.0 (100)</td><td align="center">0.6 (33)</td><td align="center">0.0 (64)</td></tr><tr><td></td><td colspan="10"><hr></hr></td></tr><tr><td></td><td align="center">grape</td><td align="center">8.0<break/>(2)</td><td align="center">8.0 (21)</td><td align="center">0.6 (64)</td><td align="center">10.0 (4)</td><td align="center">10.0 (5)</td><td align="center">2.0 (11)</td><td align="center">4.0<break/>(1)</td><td align="center">6.0<break/>(1)</td><td align="center">0.0 (64)</td></tr><tr><td colspan="11"><hr></hr></td></tr><tr><td align="center">NMR tumor</td><td align="center">all</td><td align="center">1.0 (80)</td><td align="center">0.5 (11)</td><td align="center">4.0 (6)</td><td align="center">4.0 (51)</td><td align="center">0.0 (100)</td><td align="center">2.0<break/>(6)</td><td align="center">0.8 (11)</td><td align="center">0.0 (100)</td><td align="center">0.3 (80)</td></tr><tr><td></td><td colspan="10"><hr></hr></td></tr><tr><td></td><td align="center">center</td><td align="center">2.0<break/>(7)</td><td align="center">0.4<break/>(6)</td><td align="center">0.8 (86)</td><td align="center">2.0 (26)</td><td align="center">0.2 (64)</td><td align="center">0.7 (41)</td><td align="center">0.0 (100)</td><td align="center">0.7 (13)</td><td align="center">0.3 (80)</td></tr><tr><td colspan="11"><hr></hr></td></tr><tr><td align="center">NMR candida</td><td align="center">1</td><td align="center">0.5 (80)</td><td align="center">0.0 (80)</td><td align="center">0.8 (80)</td><td align="center">0.0 (100)</td><td align="center">0.0 (100)</td><td align="center">0.8 (41)</td><td align="center">0.4 (64)</td><td align="center">0.0 (100)</td><td align="center">0.4 (9)</td></tr><tr><td></td><td colspan="10"><hr></hr></td></tr><tr><td></td><td align="center">2</td><td align="center">0.4 (80)</td><td align="center">0.9 (64)</td><td align="center">0.0 (80)</td><td align="center">2.0 (64)</td><td align="center">0.4 (26)</td><td align="center">0.0 (100)</td><td align="center">2.0 (21)</td><td align="center">1.0 (21)</td><td align="center">0.4 (41)</td></tr><tr><td></td><td colspan="10"><hr></hr></td></tr><tr><td></td><td align="center">3</td><td align="center">0.0 (100)</td><td align="center">0.0 (100)</td><td align="center">0.0 (80)</td><td align="center">2.0 (80)</td><td align="center">0.6 (80)</td><td align="center">2.0 (26)</td><td align="center">2.0 (80)</td><td align="center">0.0 (100)</td><td align="center">0.7 (41)</td></tr><tr><td></td><td colspan="10"><hr></hr></td></tr><tr><td></td><td align="center">4</td><td align="center">0.8 (80)</td><td align="center">0.0 (100)</td><td align="center">0.0 (80)</td><td align="center">2.0 (80)</td><td align="center">1.0 (80)</td><td align="center">2.0 (64)</td><td align="center">0.7 (33)</td><td align="center">0.0 (100)</td><td align="center">0.3 (32)</td></tr><tr><td></td><td colspan="10"><hr></hr></td></tr><tr><td></td><td align="center">5</td><td align="center">0.0 (100)</td><td align="center">0.0 (100)</td><td align="center">0.7 (80)</td><td align="center">0.0 (100)</td><td align="center">0.4 (80)</td><td align="center">1.0 (64)</td><td align="center">0.7 (64)</td><td align="center">0.7 (80)</td><td align="center">0.4 (21)</td></tr></tbody></table><table-wrap-foot><p>Significance of accuracy improvement with feature selection as compared to using the full set of features; and percentage of original features used in a classification that has maximum accuracy (in parentheses). The significance is specified by -log10(p), where p is the p-value of a paired Wilcoxon test on the 100 hold-outs of the cross-validation (see text). For comparison, -log(0.05) = 1.3 and -log(0.001) = 3; the value of 6.0 reported for <italic>MIR BSE binned </italic>in the second row of the first column corresponds to a highly significant improvement in classification accuracy, corresponding to a p-value of 10<sup>-6</sup>.</p></table-wrap-foot></table-wrap><p>Random forest, the only nonlinear classifier applied, performed slightly better than the linear classifiers on the unselected data sets (<italic>BSE </italic>and <italic>wine </italic>data, Fig. <xref ref-type="fig" rid="F5">5</xref>), but improved only moderately in the course of the feature selection (Fig. <xref ref-type="fig" rid="F5">5</xref>, Table <xref ref-type="table" rid="T3">3</xref>: p-value &#x0003e; 10<sup>-3</sup>). Given that random forest performs well on the unselected data sets, and that little or no benefit is incurred by an additional explicit feature selection (Table <xref ref-type="table" rid="T2">2</xref>, Fig. <xref ref-type="fig" rid="F5">5</xref>), it is apparent that an implicit feature selection is at work and performs well when training the random forest classifier. Ultimately, however, the random forest classifier was surpassed in performance by any of the regularized linear methods on all data sets (Table <xref ref-type="table" rid="T2">2</xref>: column 9 vs. column 7&#x02013;8). This rather weak classification performance of the random forest may be seen in line with [<xref ref-type="bibr" rid="B20">20</xref>], but contrasts results of e.g. [<xref ref-type="bibr" rid="B10">10</xref>,<xref ref-type="bibr" rid="B18">18</xref>] using random forest in the classification of microarrays, similar to spectra in their high dimensionality of their feature vectors. Few differences could be observed between D-PLS and D-PCR classification. Among the different feature selection strategies, the Wilcoxon-test and the Gini importance performed better on average than the iterated selection based on the regression coefficients (Fig. <xref ref-type="fig" rid="F5">5</xref>, Table <xref ref-type="table" rid="T2">2</xref>), with slightly better classification results for the Gini importance (Table <xref ref-type="table" rid="T2">2</xref>). Overall, while the Gini importance was preferable in feature selection, the chemometric methods performed better than random forest in classification, in spite of their limitation to model linear dependencies only.</p><p>The two linear classifiers of this study generally seek for subspaces <italic>c</italic><sub><italic>k </italic></sub>maximizing the variance <italic>Var </italic>of the explanatory variables <italic>X </italic>in the subspace <italic>c</italic></p><p><disp-formula><graphic xlink:href="1471-2105-10-213-i7.gif"/></disp-formula></p><p>in case of PCR or the product of variance and the (squared) correlation <italic>Corr</italic></p><p><disp-formula><graphic xlink:href="1471-2105-10-213-i8.gif"/></disp-formula></p><p>with the response <italic>y </italic>in case of PLS [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B3">3</xref>]. Thus, for a better understanding of D-PCR and D-PLS, both <italic>Corr</italic>(<italic>x</italic>, <italic>y</italic>) and <italic>Var</italic>(<italic>x</italic>) were plotted for individual channels and for individual learning tasks in Fig. <xref ref-type="fig" rid="F6">6</xref> (with the absolute value of the coefficients of <italic>c </italic>encoded by the size of the circles in Fig. <xref ref-type="fig" rid="F6">6</xref>). On data sets which did not benefit greatly from the feature selection, we observed variance and correlation to be maximal in those variables which were finally assigned the largest coefficients in the regression (indicated by the size of the black circles in Fig. <xref ref-type="fig" rid="F6">6</xref>). Conversely, in data sets where a feature selection was required, features with high variance but only moderate relevance to the classification problem (as indicated by a low univariate correlation or multivariate Gini importance) were frequently present in the unselected data (Fig. <xref ref-type="fig" rid="F6">6</xref>, black dots). This might be seen as a likely reason for the bad performance of D-PCR and D-PLS when used without preceding feature selection on the <italic>BSE </italic>and <italic>wine </italic>data: Here the selection process allowed to identify those features where variance coincided with class-label correlation (Fig. <xref ref-type="fig" rid="F6">6</xref>, red circles), leading to a similar situation in the subsequent regression as for those data sets where a feature selection was not required (Fig. <xref ref-type="fig" rid="F6">6</xref>, compare subselected features indicated red in the left and central row with features in the right row).</p><fig position="float" id="F6"><label>Figure 6</label><caption><p><bold>Channel-wise variance of each feature (horizontal axis) and its correlation with the dependent variable (vertical axis)</bold>. For the data sets of the left and the central column, a feature selection was not required for optimal performance, while the data sets shown in the right columns benefitted from a feature selection. Circle diameter indicates magnitude of the coefficient in the PLS regression. In the right column selected features are shown by red circles, while (the original values of) eliminated features are indicated by black dots. Relevant features show both a high variance and correlation with the class labels.</p></caption><graphic xlink:href="1471-2105-10-213-6"/></fig><p>In summary, observing that the degree of ill-posedness is not in itself an indicator for a required feature selection preceding a constrained classification, it might be argued that non-discriminative variance &#x02013; hindering the identification of the optimal subspace in PCR, and disturbing the optimal trade-off between correlation and variation in PLS &#x02013; may be a reason for the constrained classifiers' failing on the unselected data and, consequently, a requirement for a feature selection in the first place.</p></sec><sec><title>Feature selection and noise processes</title><p>The first experiment advocated the use of the Gini importance for a feature selection preceding a constrained regression for some data sets. Thus, and in the light of the unexpectedly weak performance of the random forest classifier, we studied the performance of the D-PLS and the random forest classifier as a function of noise processes which can be observed in spectral data (see Methods section for details) to identify optimal situations for the joint use of explicit and implicit feature selection.</p><p>In this second experiment, random forest proved to be highly robust against the introduction of "local" noise, i.e. against noise processes affecting few spectral channels only, corresponding to spurious peaks or variant spectral regions which are irrelevant to the classification task (both on the synthetic bivariate classification problem, Figs. <xref ref-type="fig" rid="F1">1</xref> left, <xref ref-type="fig" rid="F7">7A</xref>; and the modified real data, Fig. <xref ref-type="fig" rid="F7">7CE</xref>). The random forest classifier was, however, unable to cope with additive global noise: Already random offsets that were fractions of the amplitude <italic>S </italic>of the spectra (Fig. <xref ref-type="fig" rid="F7">7DF</xref>; <italic>S </italic>= 10<sup>-2</sup>) resulted in a useless classification by the random forest. As global additive noise stretches the data along the high dimensional equivalent of the bisecting line (Fig. <xref ref-type="fig" rid="F1">1</xref>), the topology of its base learners may be a disadvantage for the random forest in classification problems as shown in Fig. <xref ref-type="fig" rid="F1">1</xref>. Single decision trees, which split feature space in a box-like manner orthogonal to the feature direction are known to be inferior to single decision trees splitting the feature space by oblique splits [<xref ref-type="bibr" rid="B34">34</xref>] (although they have a considerable computational advantage). Random offsets often occur in spectral data, for example resulting from broad underlying peaks or baselines, or from the normalization to spectral regions that turn out to be irrelevant to the classification problem. Thus, one might argue that the "natural" presence of a small amount of such noise may lead to the rather weak overall performance of the random forest observed in the first experiment (Table <xref ref-type="table" rid="T2">2</xref>, Fig. <xref ref-type="fig" rid="F5">5</xref>).</p><fig position="float" id="F7"><label>Figure 7</label><caption><p><bold>The effect of different noise processes on the performance of a random forest (green triangles) and a PLS classification (red circles)</bold>. In the left column, feature vectors are augmented by a random variable, which is subsequently rescaled according to a factor S (horizontal axis), thus introducing non-discriminatory variance to the classification problem. In the right column, a random variable scaled by factor S is added as constant offset to the feature vectors, increasing the correlation between features (see text for details). Shown are results on the basis of the bivariate classification problem of Fig. 1 (top row), the <italic>NMR candida 2 </italic>data (middle), and the <italic>BSE binned </italic>data (below).</p></caption><graphic xlink:href="1471-2105-10-213-7"/></fig><p>Partial least squares performed slightly better than random forests on all three data sets at the outset (Fig. <xref ref-type="fig" rid="F7">7</xref>). In contrast to the random forest, PLS was highly robust against global additive noise: On the synthetic classification problem &#x02013; being symmetric around the bisecting line &#x02013; the random offsets did not influence the classification performance at all (Fig. <xref ref-type="fig" rid="F7">7B</xref>). On the real data &#x02013; with more complex classification tasks &#x02013; the D-PLS classification still showed to be more robust against random offsets than the random forest classifier (Fig. <xref ref-type="fig" rid="F7">7DF</xref>). Conversely, local noise degraded the performance of the D-PLS classification (Fig. <xref ref-type="fig" rid="F6">6ACE</xref>, although for rather large values of <italic>S </italic>only). The D-PLS classifier seemed to be perfectly adapted to additive noise &#x02013; splitting classes at arbitrary oblique directions &#x02013; but its performance was degraded by a large contribution of non-discriminatory variance to the classification problem (Figs. <xref ref-type="fig" rid="F6">6</xref> &#x00026;<xref ref-type="fig" rid="F7">7ACE</xref>).</p><p>In the presence of increasing additive noise, both univariate and multivariate (i.e., the Gini importance) feature importance measures lost their power to discriminate between relevant and random variables at the end (Fig. <xref ref-type="fig" rid="F8">8DF</xref>), with the Gini importance retaining discriminative power somewhat longer finally converging to a similar value for all three variables correlating well with a random classification and an (equally) random assignment of feature importance (Fig. <xref ref-type="fig" rid="F8">8D</xref>). When introducing a source of local random noise and normalizing the data accordingly, the univariate tests degraded to random output (Fig. <xref ref-type="fig" rid="F8">8E</xref>), while the Gini importance measure (Fig. <xref ref-type="fig" rid="F8">8CE</xref>) virtually ignored the presence and upscaling of the non-discriminatory variable (as did the random forest classifier in Fig. <xref ref-type="fig" rid="F7">7ACE</xref>).</p><fig position="float" id="F8"><label>Figure 8</label><caption><p><bold>The effect of different noise processes on the performance of the feature selection methods in the synthetic bivariate classification problem illustrated in Fig. 1</bold>. In the left column feature vectors are extended by a random variable scaled by S, in the right column a random offset of size S is added to the feature vectors. Top row: classification accuracy of the synthetic two-class problem (as in Fig. 7, for comparison); second row: multivariate Gini importance, bottom row: p-values of univariate t-test. The black lines correspond to the values of the two features spanning the bivariate classification task (Fig. 1), the blue dotted line corresponds to the third feature in the synthetic data set, the random variable. The performance of the random forest remains nearly unchanged even under the presence of a strong source of "local" noise for high values of S.</p></caption><graphic xlink:href="1471-2105-10-213-8"/></fig></sec><sec><title>Feature selection using the Gini importance</title><p>Overall, we observed that the random forest classifier &#x02013; with the non-oblique splits of its base learner &#x02013; may not be the optimal choice in the classification of spectral data. For feature selection, however, its Gini importance allowed to rank non-discriminatory features low and to remove them early on in a recursive feature elimination. This desirable property is due to the Gini importance being based on a rank order measure which is <italic>invariant to the scaling </italic>of individual variables and unaffected by non-discriminatory variance that does disturb D-PCR and D-PLS. Thus, for a constrained classifier requiring a feature selection due to the specificities of the classification problem (Table <xref ref-type="table" rid="T2">2</xref>, Fig. <xref ref-type="fig" rid="F5">5</xref>), the Gini feature importance might be a preferable ranking criterion: as a <italic>multivariate </italic>feature importance, it is considering conditional higher-order interactions between the variables when measuring the importance of certain spectral regions, providing a better ranking criterion than a univariate measure used here and in similar tasks elsewhere [<xref ref-type="bibr" rid="B9">9</xref>,<xref ref-type="bibr" rid="B10">10</xref>].</p><p>A comparison of the computing times of the different feature selection and classification approaches (Table <xref ref-type="table" rid="T4">4</xref>) shows that the computational costs for using the Gini importance is comparable to the cost of using the other multivariate feature selection criterion tested in this study. On average the computing time was no more than twice as long as for the more basic univariate importance measures.</p><table-wrap position="float" id="T4"><label>Table 4</label><caption><p>Computing times.</p></caption><table frame="hsides" rules="groups"><thead><tr><td></td><td></td><td align="center" colspan="3">no selection</td><td align="center" colspan="3">univariate selection</td><td align="center" colspan="3">Multivariate selection (Gini importance)</td><td align="center" colspan="3">multivariate selection (PLS/PC)</td></tr><tr><td></td><td></td><td colspan="12"><hr></hr></td></tr><tr><td></td><td></td><td align="center">PLS</td><td align="center">PC</td><td align="center">RF</td><td align="center">PLS</td><td align="center">PC</td><td align="center">RF</td><td align="center">PLS</td><td align="center">PC</td><td align="center">RF</td><td align="center">PLS</td><td align="center">PC</td><td align="center">RF</td></tr></thead><tbody><tr><td align="center">MIR BSE</td><td align="center">orig</td><td align="center">5.7</td><td align="center">11.1</td><td align="center">9.9</td><td align="center">46.4</td><td align="center">53.9</td><td align="center">46.8</td><td align="center">88.8</td><td align="center">97.0</td><td align="center">91.5</td><td align="center">87.9</td><td align="center">92.4</td><td align="center">88.0</td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">binned</td><td align="center">2.8</td><td align="center">3.2</td><td align="center">3.1</td><td align="center">13.6</td><td align="center">14.7</td><td align="center">15.9</td><td align="center">26.1</td><td align="center">27.1</td><td align="center">29.0</td><td align="center">28.7</td><td align="center">29.6</td><td align="center">31.5</td></tr><tr><td colspan="14"><hr></hr></td></tr><tr><td align="center">MIR wine</td><td align="center">French</td><td align="center">8.8</td><td align="center">7.8</td><td align="center">2.4</td><td align="center">26.6</td><td align="center">21.8</td><td align="center">7.7</td><td align="center">47.0</td><td align="center">45.9</td><td align="center">33.5</td><td align="center">17.2</td><td align="center">14.7</td><td align="center">7.4</td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">grape</td><td align="center">12.1</td><td align="center">10.3</td><td align="center">2.5</td><td align="center">28.9</td><td align="center">22.3</td><td align="center">8.0</td><td align="center">54.0</td><td align="center">47.6</td><td align="center">33.5</td><td align="center">15.8</td><td align="center">13.1</td><td align="center">6.5</td></tr><tr><td colspan="14"><hr></hr></td></tr><tr><td align="center">NMR tumor</td><td align="center">all</td><td align="center">0.3</td><td align="center">0.4</td><td align="center">0.4</td><td align="center">1.4</td><td align="center">1.2</td><td align="center">2.1</td><td align="center">2.9</td><td align="center">2.7</td><td align="center">3.6</td><td align="center">3.6</td><td align="center">3.4</td><td align="center">4.3</td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">center</td><td align="center">0.2</td><td align="center">0.2</td><td align="center">0.2</td><td align="center">1.1</td><td align="center">0.8</td><td align="center">1.1</td><td align="center">2.2</td><td align="center">1.9</td><td align="center">2.1</td><td align="center">2.1</td><td align="center">1.8</td><td align="center">2.0</td></tr><tr><td colspan="14"><hr></hr></td></tr><tr><td align="center">NMR candida</td><td align="center">1</td><td align="center">4.6</td><td align="center">8.8</td><td align="center">7.7</td><td align="center">22.4</td><td align="center">41.2</td><td align="center">37.1</td><td align="center">43.5</td><td align="center">62.5</td><td align="center">61.1</td><td align="center">59.8</td><td align="center">78.4</td><td align="center">75.4</td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">2</td><td align="center">3.7</td><td align="center">4.8</td><td align="center">3.8</td><td align="center">18.0</td><td align="center">22.0</td><td align="center">19.4</td><td align="center">34.5</td><td align="center">38.5</td><td align="center">37.3</td><td align="center">36.3</td><td align="center">40.3</td><td align="center">37.9</td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">3</td><td align="center">3.7</td><td align="center">4.7</td><td align="center">3.7</td><td align="center">17.4</td><td align="center">20.1</td><td align="center">17.9</td><td align="center">33.4</td><td align="center">36.0</td><td align="center">34.7</td><td align="center">34.6</td><td align="center">37.8</td><td align="center">35.1</td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">4</td><td align="center">3.9</td><td align="center">5.1</td><td align="center">4.8</td><td align="center">18.7</td><td align="center">23.4</td><td align="center">24.3</td><td align="center">36.0</td><td align="center">40.5</td><td align="center">60.5</td><td align="center">41.6</td><td align="center">46.2</td><td align="center">47.0</td></tr><tr><td></td><td colspan="13"><hr></hr></td></tr><tr><td></td><td align="center">5</td><td align="center">3.5</td><td align="center">3.9</td><td align="center">2.6</td><td align="center">31.9</td><td align="center">32.4</td><td align="center">27.0</td><td align="center">62.6</td><td align="center">63.0</td><td align="center">60.0</td><td align="center">58.3</td><td align="center">43.4</td><td align="center">38.5</td></tr></tbody></table><table-wrap-foot><p>The table reports the runtime for the different feature selection and classification approaches, and the different data sets (on a 2 GHz personal computer with 2 GB memory). Values are given in minutes, for a ten-fold cross-validation and with parameterisations as used for the results shown in Tables 2 and 3. For all methods, a univariate feature selection takes about five times as long as a classification of the same data set without feature selection. Both multivariate feature selection approaches require approximately the same amount of time for a given data set and classifier. Their computing time is no more than twice as long as in a recursive feature elimination based on a univariate feature importance measure.</p></table-wrap-foot></table-wrap></sec></sec><sec><title>Conclusion</title><p>In the joint application of the feature selection and classification methods on spectral data neither the random forests classifier using the Gini importance in a recursive feature selection, nor a constrained regression without feature selection were the optimal choice for classification. Random forest showed to be robust against single noisy features with a large amount of non-discriminatory variance. Unfortunately it also showed to be highly sensible to random offsets in the feature vector, a common artefact in spectral data. D-PLS was capable of dealing with such offsets, although it failed in the presence of non-discriminatory variance in single, highly variable features. The removal of such irrelevant &#x02013; or even misleading &#x02013; predictors was crucial in the application of the constrained classifiers tested in this study. Overall, the combined application of Gini importance in a recursive feature elimination together with a D-PLS classification was either the best approach or &#x02013; in terms of statistical significance &#x02013; comparable to the best in all classification tasks, and may be recommended for the separation of binary, linearly separable data.</p><p>The results also suggest that when using a constrained learning method &#x02013; such as the D-PLS or D-PCR classifier as in this study &#x02013; the main purpose of a feature selection is the removal of few "noisy" features with a large amount of variance, but little importance to the classification problem. Then, the feature elimination is a first step in the regularization of a classification task, removing features with non-discriminatory variance, and allowing for a better regularization and implicit dimension reduction by the subsequent classifier. Considering the similarity of PLS, ridge regression and continuum regression [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B3">3</xref>,<xref ref-type="bibr" rid="B35">35</xref>] &#x02013; all of them trading correlation with class labels, and variance of the data for a regularization &#x02013; one might expect this to be a general feature for these constrained regression methods.</p><p>Only binary classifications tasks were studied here, but one may expect that results generalize to multi-class problems as well when using, for example, penalized mixture models in place of a D-PLS classification. It might be worthwhile to test whether using a constrained classifier in the final classification step of a recursive feature selection is able to increase the classification performance on other data as well, for example on microarrays where a recent study [<xref ref-type="bibr" rid="B20">20</xref>] reported of a general advantage of support vector machines with RBF-kernel over the random forest classifier.</p><p>Of course, rather than advocating a hybrid method using random forest for feature selection and a constrained linear classifier to predict class membership, it might be advantageous to adapt the random forest classifier itself to fit the properties of spectral data in an optimal fashion. For individual tree-like classifiers, a large body of literature about trees using such non-orthogonal, linear splits in their nodes is available [<xref ref-type="bibr" rid="B34">34</xref>] and may be used for such an adaption of the random forest classifier.</p></sec><sec sec-type="methods"><title>Methods</title><p>In a first experiment, we systematically evaluated the joint use of different feature selection and classification methods, on a number of different spectral data sets. In a second, we looked into the behaviour of the random forest and D-PLS classifier on synthetically modified data, to understand specific properties of these methods when applied to spectra.</p><sec><title>Experiment 1: Joint feature selection and classification</title><p>In general, feature selection is a concern in both regression (prediction of a continuous response) and in classification (prediction of two or more categories). Here, we confined ourselves to binary classification tasks. Our experiments were based on four different data sets available to us from different studies [<xref ref-type="bibr" rid="B36">36</xref>-<xref ref-type="bibr" rid="B39">39</xref>], providing &#x02013; with different preprocessing, labels, or dichotomous sub-problems &#x02013; eleven binary classification tasks (Table <xref ref-type="table" rid="T2">2</xref>).</p><sec><title>Classification and feature selection methods</title><p>Three different feature selection approaches were applied to the data in an explicit recursive feature elimination (Table <xref ref-type="table" rid="T1">1</xref>), together with the three following classifiers: linear discriminant principal component (D-PCR) and partial least squares (D-PLS) classification (using [<xref ref-type="bibr" rid="B40">40</xref>]) and the nonlinear random forest classifier (RF) (using [<xref ref-type="bibr" rid="B41">41</xref>]). While trees in the random forest allow for a classification via majority votes and a binary decision, D-PCR and D-PLS classification were used with a predefined and fixed threshold, i.e. a score of 0.5 intermediate to the trained class values 0 and 1, using balanced classes during training and for the test (see below).</p><p>As univariate feature selection measure, the p-values of channel-wise Wilcoxon-tests for class differences were used to rank the features and to allow for a filtering of features prior to the D-PLS and D-PCR classification. In the two multivariate feature selection procedures applied, variables were recursively eliminated either according to smallest PLS or PC regression coefficient (as in [<xref ref-type="bibr" rid="B5">5</xref>], although without stability test), or according to smallest Gini importance value. In the latter case the actual classification in the selected subspace was performed not only by a RF as in [<xref ref-type="bibr" rid="B15">15</xref>-<xref ref-type="bibr" rid="B18">18</xref>], but also by the linear classifiers. In total, seven different combinations of feature selection and classification methods were applied to the data (Table <xref ref-type="table" rid="T2">2</xref>). The classification of the data without any feature selection was tested as well. For the results shown in the last column of Tables <xref ref-type="table" rid="T2">2</xref> and <xref ref-type="table" rid="T3">3</xref> &#x02013; i.e. the combination of feature selection using regression coefficients and a subsequent classification by RF &#x02013; PLS was used in feature selection. For the sake of computational simplicity, all multivariate feature selection measures were optimized using their own cost function and not in a joint loop with the subsequent classifier (using a cross-validated least-squares-error for PLS and PCR regression coefficients and the out-of-bag classification error of the RF for Gini importance, optimized over the same parameter spaces as the respective classifiers). For both univariate filters and multivariate wrappers, 20% of the remaining features were removed in each iteration step. Prior to classification all data was subject to L<sub>1 </sub>normalization, i.e. to a normalization of the area under the spectrum in a predefined spectral region.</p></sec><sec><title>Data</title><p>Data set one, the <italic>BSE </italic>data set, originates from a study concerning a conceivable ante-mortem test for bovine spongiform encephalopathy (BSE). Mid-infrared spectra of <italic>N = 200 </italic>dried bovine serum samples (<italic>N</italic><sub><italic>pos </italic></sub>= <italic>95</italic>, <italic>N</italic><sub><italic>neg </italic></sub>= <italic>105</italic>) were recorded in the spectral range of 400&#x02013;4000 <italic>cm</italic><sup>-1 </sup>with <italic>P = 3629 </italic>data points per spectrum. Details of the sample preparation and of the acquisition of spectra are reported in Refs. [<xref ref-type="bibr" rid="B14">14</xref>,<xref ref-type="bibr" rid="B31">31</xref>]. The same spectra were used in a second binary classification task after a smoothing and downsampling ("binning"), and thus by reducing the number of data points per spectrum to <italic>P</italic><sub><italic>red </italic></sub>= <italic>1209</italic>.</p><p>Data set two, the <italic>wine </italic>data set, comprised <italic>N = 71 </italic>mid-infrared spectra with a length of <italic>P = 3445 </italic>data points from the spectral region of 899&#x02013;7496 <italic>cm</italic><sup>-1</sup>, sampled at a resolution of approx. 4 <italic>cm</italic><sup>-1 </sup>interpolated to 2 <italic>cm</italic><sup>-1</sup>, originating from the analysis of 63 different wines using an automated MIRALAB analyzer with AquaSpec flow cell. In the preprocessing a polynomial filter (Savitzky-Golay, length 9) of second order was applied to the spectra. Labels assigned to these data were the type of grape (<italic>N</italic><sub><italic>red </italic></sub>= <italic>30</italic>, <italic>N</italic><sub><italic>white </italic></sub>= <italic>41</italic>) in a first learning task and an indicator of the geographic origin of the wine (<italic>N</italic><sub><italic>French </italic></sub>= <italic>26</italic>, <italic>N</italic><sub><italic>World </italic></sub>= <italic>45</italic>) in a second.</p><p>Data set three, the <italic>tumor </italic>data set, comprised <italic>N = 278 </italic>in vivo 1H-NMR spectra with a length of <italic>P = 101 </italic>data points from the spectral region between approximately 1.0 <italic>ppm </italic>and 3.5 <italic>ppm</italic>, originating from 31 magnetic resonance spectroscopic images of 31 patients, acquired at 1.5 T with an echo time of 135 <italic>ms </italic>in the pre-therapeutic and post-operative diagnostics of (recurrent) brain tumor (<italic>N</italic><sub><italic>healthy </italic></sub>= <italic>153</italic>, <italic>N</italic><sub><italic>tumor border </italic></sub>= <italic>72, N</italic><sub><italic>tumor center </italic></sub>= <italic>53</italic>) [<xref ref-type="bibr" rid="B31">31</xref>,<xref ref-type="bibr" rid="B32">32</xref>]. Two binary groupings were tested, either discriminating healthy vs. both tumor groups (<italic>tumor all</italic>), or the spectral signature of the tumor center vs. the remaining spectra (<italic>tumor center</italic>).</p><p>Data set four, the <italic>candida </italic>data set, comprised <italic>N = 581 </italic><sup>1</sup>H-NMR spectra of cell suspensions with a length of <italic>P </italic>= 1500 data points in between 0.35&#x02013;4 ppm, originating from a chemotaxonomic classification of yeast species (Candida albicans, C. glabrata, C. krusei, C. parapsilosis, and C. tropicalis). A subset of the data was originally published in [<xref ref-type="bibr" rid="B34">34</xref>]. Its five different subgroups of sizes <italic>N = {175, 109, 101, 111, 85} </italic>allowed to define five different binary subproblems ("one-against-all").</p></sec><sec><title>Comparison</title><p>In the evaluation, 100 training and 100 test sets were sampled from each of the available data sets, in a ten times repeated ten-fold cross validation [<xref ref-type="bibr" rid="B42">42</xref>,<xref ref-type="bibr" rid="B43">43</xref>], following the overall test design in [<xref ref-type="bibr" rid="B19">19</xref>]. In order to obtain equal class priors both in training and testing, the larger of the two groups of the binary problems was subsampled to the size of the smaller if necessary. Where dependence between observations was suspected, e.g. in the <italic>tumor </italic>data where more than one spectrum originated for each patient, the cross-validation was stratified to guarantee that all spectra of a correlated subset were exclusively assigned to either the training or the test data [<xref ref-type="bibr" rid="B42">42</xref>,<xref ref-type="bibr" rid="B43">43</xref>].</p><p>The random forest parameters were optimized in logarithmic steps around their default values [<xref ref-type="bibr" rid="B41">41</xref>] (using 300 trees, and a random subspace with dimensionality equal to the rounded value of the square of the number of features) according to the out-of-bag error of the random forest, while the number of latent variables <italic>&#x003b3; </italic>in the linear classifiers was determined by an internal five-fold cross-validation for each subset, following the 1<italic>&#x003c3; </italic>rule for choosing the <italic>&#x003b3; </italic>at the intersection between the least error (at <italic>&#x003b3;</italic><sub><italic>opt</italic></sub>) plus an interval corresponding to the 1<italic>&#x003c3; </italic>standard deviation at <italic>&#x003b3;</italic><sub><italic>opt</italic></sub>, and the mean accuracy.</p><p>The classification accuracy was averaged over all 100 test results and used as performance measure in the comparison of the different methods. While all feature selection and all optimization steps in the classification were performed utilizing the training data only, test result were recorded for all feature subsets obtained during the course of feature selection (Fig. <xref ref-type="fig" rid="F4">4</xref>). To verify significant differences between the test results, a paired Cox-Wilcoxon test was used on the accuracies of the 100 test sets as proposed in [<xref ref-type="bibr" rid="B42">42</xref>,<xref ref-type="bibr" rid="B43">43</xref>]. Such paired comparisons were performed for each classifier between the classification result obtained for the full set of features, and the best result when applied in conjunction with a selection method (i.e. the results with highest classification accuracy in the course of feature selection). Feature selection approaches leading to a significant increase in classification performance were indicated accordingly (Table <xref ref-type="table" rid="T2">2</xref>, indicated by stars). Once the best feature selection and classification approach had been identified for a data set (as defined by the highest classification accuracy in a row in Table <xref ref-type="table" rid="T2">2</xref>), it was compared against all other results on the same data set. Results which were indistinguishable from this best approach (no statistical difference at a 5% level) were indicated as well (Table <xref ref-type="table" rid="T2">2</xref>, indicated by bold values).</p></sec></sec><sec><title>Experiment 2: PLS and RF classification as a function of specific data properties</title><p>D-PLS reportedly benefits from an explicit feature selection on some data sets [<xref ref-type="bibr" rid="B33">33</xref>]. Random forest reportedly performed well in classification tasks with many features and few samples [<xref ref-type="bibr" rid="B15">15</xref>-<xref ref-type="bibr" rid="B18">18</xref>], but was outperformed by standard chemometrical learning algorithms when used to classify spectral data. Thus, to identify reasons for these differences and to corroborate findings from experiment one, we decided to study the performance of both methods in dependence of two noise processes which are specific to spectral data.</p><sec><title>Noise processes</title><p>We identified two sources of unwanted variation (noise processes) which can be observed in spectral data, and which can jeopardize the performance of a classifier.</p><p>First, there are processes affecting few, possibly adjacent, spectral channels only. Examples for such changes in the spectral pattern are insufficiently removed peaks and slight peak shifts (magnetic resonance spectroscopy), the presence of additional peaks from traces of unremoved components in the analyte, or from vapour in the light beam during acquisition (infrared spectroscopy). Identifying and removing spectral channels affected by such processes is often the purpose of explicit feature selection. We refer to this kind of noise as "local noise" in the following, where the locality refers to the adjacency of channels along the spectral axis.</p><p>Second, there are processes affecting the spectrum as a whole. Examples for such noise processes may be the presence (or absence) of broad baselines, resulting in a random additional offset in the spectrum. Variation may also result from differences in the signal intensities due to changes in the concentration of the analyte, variation in reflectance or transmission properties of the sample (infrared spectroscopy), or the general signal amplitude from voxel bleeding and partial volume effects (magnetic resonance spectroscopy), leading to a scaling of the spectrum as a whole, and &#x02013; after normalization &#x02013; to random offsets in the spectrum. Such processes increase the nominal correlation between features and are the main reason for the frequent use of high-pass filters in the preprocessing of spectral data (Savitzky-Golay filter, see above). It might be noted that this noise does not have to offset the spectrum as a whole to affect the classification performance significantly, but may only modify those spectral regions which turn out to be relevant to the classification task. Nevertheless, we refer to this kind of noise as "global noise" here.</p></sec><sec><title>Modified and synthetic data sets</title><p>For visualization (Fig <xref ref-type="fig" rid="F1">1</xref>, left), we modelled a synthetic two-class problem, by drawing 2*400 samples from two bivariate normal distributions (centred at (0,1) and (1,0), respectively, with standard deviation 0.5). The two features for the two-dimensional classification task were augmented by a third feature comprising only random noise (normally distributed, centred at 0, standard deviation 0.5), resulting in a data set with N = 800, P = 3 and balanced classes. To mimic local noise, we rescaled the third, random feature by a factor S, for S = 2<sup>{0,1,...,20}</sup>. In real spectra one might expect S &#x02013; the ratio between the amplitude of a variable that is relevant to the classification problem, and a larger variable introducing non-discriminatory variance only &#x02013; to be of several orders of magnitude. Here, changing S gradually increased the variance of the third feature and the amount of non-discriminatory variance in the classification problem. To mimic global noise, we added a constant offset (normally distributed, centred at 0, standard deviation 0.5), also scaled by S = 2<sup>{0,1,...,20}</sup>, to the features of every sample as an offset. This increased the correlation between the features and, along S, gradually stretched the data along the bisecting line (Fig. <xref ref-type="fig" rid="F1">1</xref>, right).</p><p>In addition to the synthetic two-class problem of Fig. <xref ref-type="fig" rid="F1">1</xref>, we modified two exemplary real data sets (<italic>candida 2 </italic>and <italic>BSE binned</italic>) by these procedures in the same way, here using S = 10<sup>{-6,-4,...,16}</sup>, using the largest amplitude of a spectrum as reference for a shift by S = 1, or a rescaling of the random feature (N(0,.5)).</p></sec><sec><title>Comparison</title><p>Gini importance and univariate importance (t-test) were calculated along S for the features of the synthetic data set. PLS and random forest classification were applied to all data sets, for all values of S, after a L<sub>2</sub>-normalization of the feature vector. (Which may be a closer match with the noise statistic than the L<sub>1 </sub>normalization used in the real data in the first experiment.) Classification accuracy was determined according to the procedure described above (Experiment 1).</p></sec></sec></sec><sec><title>Authors' contributions</title><p>BHM performed the design of the study and drafted the manuscript. FAH contributed significantly to the manuscript. RM, UH, PB, WP acquired data for the study. All authors participated in the analysis of the results. BHM, BMK, WP, FAH conceived of the study and participated in its coordination. All authors read and approved the final manuscript.</p></sec></body><back><ack><sec><title>Acknowledgements</title><p>BHM acknowledges support from the German Academy of Sciences Leopoldina through the Leopoldina Fellowship Programme (LPDS 2009-10). FAH acknowledges support by the German Research Foundation (DFG HA-4364).</p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Guyon</surname><given-names>I</given-names></name><name><surname>Elisseeff</surname><given-names>A</given-names></name></person-group><article-title>An introduction to variable and feature selection</article-title><source>J Mach Learn Res</source><year>2003</year><volume>3</volume><fpage>1157</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1162/153244303322753616</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Stone</surname><given-names>M</given-names></name><name><surname>J</surname><given-names>R</given-names></name><collab>Brooks Continuum regression</collab></person-group><article-title>Cross-validated sequentially constructed prediction embracing ordinary least squares, partial least squares and principal components regression</article-title><source>J Roy Stat Soc B (Meth)</source><year>1990</year><volume>52</volume><fpage>237</fpage><lpage>269</lpage></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>IE</given-names></name><name><surname>Friedman</surname><given-names>JH</given-names></name></person-group><article-title>A statistical view of some Chemometrics regression tools</article-title><source>Technometrics</source><year>1993</year><volume>35</volume><fpage>109</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.2307/1269656</pub-id></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bylesj&#x000f6;</surname><given-names>M</given-names></name><name><surname>Rantalainen</surname><given-names>M</given-names></name><name><surname>Nicholson</surname><given-names>JK</given-names></name><name><surname>Holmes</surname><given-names>E</given-names></name><name><surname>Trygg</surname><given-names>J</given-names></name></person-group><article-title>K-OPLS package: Kernel-based orthogonal projections to latent structures for prediction and interpretation in feature space</article-title><source>BMC Bioinformatics</source><year>2008</year><volume>9</volume><fpage>106</fpage><pub-id pub-id-type="pmid">18284666</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-9-106</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Westad</surname><given-names>F</given-names></name><name><surname>Martens</surname><given-names>H</given-names></name></person-group><article-title>Variable selection in near infrared spectroscopy based on significance testing in partial least squares regression</article-title><source>J Near Infrared Spectrosc</source><year>2000</year><volume>117</volume><fpage>117</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1255/jnirs.271</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Nadler</surname><given-names>B</given-names></name><name><surname>Coifman</surname><given-names>RR</given-names></name></person-group><article-title>The prediction error in CLS and PLS: the importance of feature selection prior to multivariate calibration</article-title><source>J Chemometrics</source><year>2005</year><volume>19</volume><fpage>107</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1002/cem.915</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Denham</surname><given-names>MC</given-names></name><name><surname>Brown</surname><given-names>PJ</given-names></name></person-group><article-title>Calibration with many variables</article-title><source>Appl Stat</source><year>1993</year><volume>42</volume><fpage>515</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.2307/2986329</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Baumann</surname><given-names>K</given-names></name><name><surname>von Korff</surname><given-names>M</given-names></name><name><surname>Albert</surname><given-names>H</given-names></name></person-group><article-title>Asystematic evaluation of the benefits and hazards of variable selection in latent variable regression. Part I. Search Algorithm, theory and simulations</article-title><source>J Chemometrics</source><year>2002</year><volume>16</volume><fpage>339</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1002/cem.730</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Eisen</surname><given-names>MB</given-names></name><name><surname>Alizadeh</surname><given-names>A</given-names></name><name><surname>Levy</surname><given-names>R</given-names></name><name><surname>Staudt</surname><given-names>L</given-names></name><name><surname>Chan</surname><given-names>WC</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name><name><surname>Brown</surname><given-names>P</given-names></name></person-group><article-title>'Gene shaving' as a method for identifying distinct sets of genes with similar expression patterns</article-title><source>Genome Biol</source><year>2000</year><volume>1</volume><fpage>3</fpage><pub-id pub-id-type="doi">10.1186/gb-2000-1-2-research0003</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>XQ</given-names></name><name><surname>Li</surname><given-names>GZ</given-names></name><name><surname>Yang</surname><given-names>JY</given-names></name><name><surname>Yang</surname><given-names>MQ</given-names></name><name><surname>Wu</surname><given-names>GF</given-names></name></person-group><article-title>Dimension reduction with redundant gene elimination for tumor classification</article-title><source>BMC Bioinformatics</source><year>2008</year><volume>9</volume><fpage>S8</fpage><pub-id pub-id-type="pmid">18541061</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-9-S6-S8</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Leardi</surname><given-names>R</given-names></name></person-group><article-title>Genetic algorithms in chemometrics and chemistry: a review</article-title><source>J Chemometrics</source><year>2001</year><volume>15</volume><fpage>559</fpage><lpage>569</lpage><pub-id pub-id-type="doi">10.1002/cem.651</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Centner</surname><given-names>V</given-names></name><name><surname>Massart</surname><given-names>DL</given-names></name><name><surname>de Noord</surname><given-names>OE</given-names></name><name><surname>de Jong</surname><given-names>S</given-names></name><name><surname>Vandeginste</surname><given-names>BM</given-names></name><name><surname>Sterna</surname><given-names>C</given-names></name></person-group><article-title>Elimination of uninformative variables for multivariate calibration</article-title><source>Anal Chem</source><year>1996</year><volume>68</volume><fpage>3851</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1021/ac960321m</pub-id></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Forina</surname><given-names>M</given-names></name><name><surname>Casolino</surname><given-names>C</given-names></name><name><surname>Millan</surname><given-names>CP</given-names></name></person-group><article-title>Iterative predictor weighting (IPW) PLS: a technique for the elimination of useless predictors in regression problems</article-title><source>J Chemometrics</source><year>1999</year><volume>13</volume><fpage>165</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1099-128X(199903/04)13:2&#x0003c;165::AID-CEM535&#x0003e;3.0.CO;2-Y</pub-id></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Random forests</article-title><source>J Mach Learn</source><year>2001</year><volume>45</volume><fpage>5</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Deng</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>H-S</given-names></name><name><surname>Tao</surname><given-names>L</given-names></name><name><surname>Sha</surname><given-names>Q</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Tsai</surname><given-names>C-J</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name></person-group><article-title>Joint analysis of two microarray gene-expression data sets to select lung adenocarcinoma marker genes</article-title><source>BMC Bioinformatics</source><year>2004</year><volume>5</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">14706121</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-5-81</pub-id></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Diaz-Uriarte</surname><given-names>R</given-names></name><name><surname>Alvarez de Andres</surname><given-names>S</given-names></name></person-group><article-title>Gene selection and classification of microarray data using random forest</article-title><source>BMC Bioinformatics</source><year>2006</year><volume>7</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="pmid">16393334</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-7-3</pub-id></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Fedorowicz</surname><given-names>A</given-names></name><name><surname>Singh</surname><given-names>H</given-names></name><name><surname>Soderholm</surname><given-names>SC</given-names></name></person-group><article-title>Application of the random forest method in studies of local lymph node assay based skin sensitization data</article-title><source>J Chem Inf Comp Sci</source><year>2005</year><volume>45</volume><fpage>952</fpage><lpage>64</lpage></citation></ref><ref id="B18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Geurts</surname><given-names>P</given-names></name><name><surname>Fillet</surname><given-names>M</given-names></name><name><surname>de Seny</surname><given-names>D</given-names></name><name><surname>Meuwis</surname><given-names>M-A</given-names></name><name><surname>Malaise</surname><given-names>M</given-names></name><name><surname>Merville</surname><given-names>M-P</given-names></name><name><surname>Wehenkel</surname><given-names>L</given-names></name></person-group><article-title>Proteomic mass spectra classification using decision tree based ensemble methods</article-title><source>Bioinformatics</source><year>2005</year><volume>21</volume><fpage>313</fpage><lpage>845</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bti494</pub-id></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Dudoit</surname><given-names>S</given-names></name><name><surname>Fridlyand</surname><given-names>J</given-names></name><name><surname>Speed</surname><given-names>TP</given-names></name></person-group><article-title>Comparison of discrimination methods for the classification of tumors using gene expression data</article-title><source>J Am Stat Assoc</source><year>2002</year><volume>97</volume><fpage>77</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1198/016214502753479248</pub-id></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Statnikov</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Aliferis</surname><given-names>CF</given-names></name></person-group><article-title>A comprehensive comparison of random forests and support vector machines for microarray-based cancer classification</article-title><source>BMC Bioinformatics</source><year>2008</year><volume>9</volume><fpage>319</fpage><pub-id pub-id-type="pmid">18647401</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-9-319</pub-id></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>KQ</given-names></name><name><surname>Ong</surname><given-names>CJ</given-names></name><name><surname>Li</surname><given-names>XP</given-names></name><name><surname>Zheng</surname><given-names>H</given-names></name><name><surname>Wilder-Smith</surname><given-names>EPV</given-names></name></person-group><article-title>A Feature Selection Method for Multi-Level Mental Fatigue EEG Classification</article-title><source>IEEE Trans Biomed Engin</source><year>2007</year><volume>54</volume><fpage>1231</fpage><lpage>1237</lpage><pub-id pub-id-type="doi">10.1109/TBME.2007.890733</pub-id></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Menze</surname><given-names>BH</given-names></name><name><surname>Petrich</surname><given-names>W</given-names></name><name><surname>Hamprecht</surname><given-names>FA</given-names></name></person-group><article-title>Multivariate feature selection and hierarchical classification for infrared spectroscopy: serum-based detection of bovine spongiform encephalopathy</article-title><source>Anal Bioanal Chem</source><year>2007</year><volume>387</volume><fpage>801</fpage><lpage>1807</lpage><pub-id pub-id-type="doi">10.1007/s00216-006-1070-5</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Granitto</surname><given-names>P</given-names></name><name><surname>Furlanello</surname><given-names>C</given-names></name><name><surname>Biasioli</surname><given-names>F</given-names></name><name><surname>Gasperi</surname><given-names>F</given-names></name></person-group><article-title>Recursive Feature Elimination with Random Forest for PTR-MS analysis of agroindustrial products</article-title><source>Chem Intell Lab Sys</source><year>2006</year><volume>83</volume><fpage>83</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1016/j.chemolab.2006.01.007</pub-id></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Svetnik</surname><given-names>V</given-names></name><name><surname>Liaw</surname><given-names>A</given-names></name><name><surname>Tong</surname><given-names>C</given-names></name><name><surname>Culberson</surname><given-names>JC</given-names></name><name><surname>Sheridan</surname><given-names>RP</given-names></name><name><surname>Feuston</surname><given-names>BP</given-names></name></person-group><article-title>Random Forest: A Classification and Regression Tool for Compound Classification and QSAR Modeling</article-title><source>J Chem Inf Comp Sci</source><year>2003</year><volume>43</volume><fpage>1947</fpage><lpage>58</lpage></citation></ref><ref id="B25"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Y</given-names></name><name><surname>Jeon</surname><given-names>Y</given-names></name></person-group><article-title>Random Forests and adaptive nearest neighbor</article-title><source>J Am Stat Assoc</source><year>2006</year><volume>101</volume><fpage>578</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1198/016214505000001230</pub-id></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Biau</surname><given-names>G</given-names></name><name><surname>Devroye</surname><given-names>L</given-names></name><name><surname>Lugosi</surname><given-names>G</given-names></name></person-group><article-title>Consistency of Random Forests and Other Averaging Classifiers</article-title><source>J Mach Learn Res</source><year>2008</year><volume>9</volume><fpage>2015</fpage><lpage>2033</lpage></citation></ref><ref id="B27"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Consistency for a simple model of random forests</article-title><source>Technical Report 670</source><year>2004</year><publisher-name>Technical report, Department of Statistics, University of California, Berkeley, USA</publisher-name></citation></ref><ref id="B28"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Strobl</surname><given-names>C</given-names></name><name><surname>Boulesteix</surname><given-names>A-L</given-names></name><name><surname>Kneib</surname><given-names>T</given-names></name><name><surname>Augustin</surname><given-names>T</given-names></name><name><surname>Zeileis</surname><given-names>A</given-names></name></person-group><article-title>Conditional variable importance for random forests</article-title><source>BMC Bioinformatics</source><year>2008</year><volume>9</volume><fpage>307</fpage><pub-id pub-id-type="pmid">18620558</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-9-307</pub-id></citation></ref><ref id="B29"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>R</given-names></name><name><surname>Tang</surname><given-names>W</given-names></name><name><surname>Wu</surname><given-names>X</given-names></name><name><surname>Fu</surname><given-names>W</given-names></name></person-group><article-title>A random forest approach to the detection of epistatic interactions in case-control studies</article-title><source>BMC Bioinformatics</source><year>2009</year><volume>10</volume><fpage>S65</fpage><pub-id pub-id-type="pmid">19208169</pub-id></citation></ref><ref id="B30"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Strobl</surname><given-names>C</given-names></name><name><surname>Boulesteix</surname><given-names>AL</given-names></name><name><surname>Zeileis</surname><given-names>A</given-names></name><name><surname>Hothorn</surname><given-names>T</given-names></name></person-group><article-title>Bias in Random Forest Variable Importance Measures: Illustrations, Sources and a Solution</article-title><source>BMC Bioinformatics</source><year>2007</year><volume>8</volume><fpage>25</fpage><pub-id pub-id-type="pmid">17254353</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-8-25</pub-id></citation></ref><ref id="B31"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sandri</surname><given-names>M</given-names></name><name><surname>Zuccoletto</surname><given-names>P</given-names></name></person-group><article-title>A bias correction algorithm for the Gini variable importance measure in classification trees</article-title><source>J Comp Graph Stat</source><year>2008</year><volume>17</volume><fpage>611</fpage><lpage>628</lpage><pub-id pub-id-type="doi">10.1198/106186008X344522</pub-id></citation></ref><ref id="B32"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Archer</surname><given-names>KJ</given-names></name><name><surname>Kimes</surname><given-names>RV</given-names></name></person-group><article-title>Empirical characterization of random forest variable importance measures</article-title><source>Comp Stat Data Anal</source><year>2008</year><volume>52</volume><fpage>2249</fpage><lpage>2260</lpage><pub-id pub-id-type="doi">10.1016/j.csda.2007.08.015</pub-id></citation></ref><ref id="B33"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gauchi</surname><given-names>J-P</given-names></name><name><surname>Chagnon</surname><given-names>P</given-names></name></person-group><article-title>Comparison of selection methods of explanatory variables in PLS regression with application to manufacturing process data</article-title><source>Chem Intell Lab Sys</source><year>2001</year><volume>58</volume><fpage>171</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1016/S0169-7439(01)00158-7</pub-id></citation></ref><ref id="B34"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Murthy</surname><given-names>SK</given-names></name><name><surname>Kasif</surname><given-names>S</given-names></name><name><surname>Salzberg</surname><given-names>S</given-names></name></person-group><article-title>A System for Induction of Oblique Decision Trees</article-title><source>J Artif Intell Res</source><year>1994</year><volume>2</volume><fpage>1</fpage><lpage>32</lpage></citation></ref><ref id="B35"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bjorkstrom</surname><given-names>A</given-names></name></person-group><article-title>A generalized view on continuum regression</article-title><source>Scand J Stat</source><year>1999</year><volume>26</volume><fpage>17</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1111/1467-9469.00134</pub-id></citation></ref><ref id="B36"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>TC</given-names></name><name><surname>Moecks</surname><given-names>J</given-names></name><name><surname>Belooussov</surname><given-names>A</given-names></name><name><surname>Cawthraw</surname><given-names>S</given-names></name><name><surname>Dolenko</surname><given-names>B</given-names></name><name><surname>Eiden</surname><given-names>M</given-names></name><name><surname>Von Frese</surname><given-names>J</given-names></name><name><surname>Kohler</surname><given-names>W</given-names></name><name><surname>Schmitt</surname><given-names>J</given-names></name><name><surname>Somorjai</surname><given-names>RL</given-names></name><name><surname>Udelhoven</surname><given-names>T</given-names></name><name><surname>Verzakov</surname><given-names>S</given-names></name><name><surname>Petrich</surname><given-names>W</given-names></name></person-group><article-title>Classification of signatures of bovine spongiform encephalopathy in serum using infrared spectroscopy</article-title><source>Analyst</source><year>2004</year><volume>129</volume><fpage>897</fpage><lpage>901</lpage><pub-id pub-id-type="pmid">15457319</pub-id><pub-id pub-id-type="doi">10.1039/b408950m</pub-id></citation></ref><ref id="B37"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Menze</surname><given-names>BH</given-names></name><name><surname>Lichy</surname><given-names>MP</given-names></name><name><surname>Bachert</surname><given-names>P</given-names></name><name><surname>Kelm</surname><given-names>BM</given-names></name><name><surname>Schlemmer</surname><given-names>H-P</given-names></name><name><surname>Hamprecht</surname><given-names>FA</given-names></name></person-group><article-title>Optimal classification of long echo time in vivo magnetic resonance spectra in the detection of recurrent brain tumors</article-title><source>NMR in Biomedicine</source><year>2006</year><volume>19</volume><fpage>599</fpage><lpage>60</lpage><pub-id pub-id-type="pmid">16642460</pub-id><pub-id pub-id-type="doi">10.1002/nbm.1041</pub-id></citation></ref><ref id="B38"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Menze</surname><given-names>BH</given-names></name><name><surname>Kelm</surname><given-names>BM</given-names></name><name><surname>Heck</surname><given-names>D</given-names></name><name><surname>Lichy</surname><given-names>MP</given-names></name><name><surname>Hamprecht</surname><given-names>FA</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Handels H, Ehrhardt J, Horsch A, Meinzer H-P, Tolxdorff T</surname></name></person-group><article-title>Machine based rejection of low-quality spectra and estimation of brain tumor probabilities from magnetic resonance spectroscopic images</article-title><source>Proceedings of BVM</source><year>2006</year><publisher-name>Springer, New York</publisher-name><fpage>31</fpage><lpage>35</lpage></citation></ref><ref id="B39"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Himmelreich</surname><given-names>U</given-names></name><name><surname>Somorjai</surname><given-names>RL</given-names></name><name><surname>Dolenko</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>OC</given-names></name><name><surname>Daniel</surname><given-names>HM</given-names></name><name><surname>Murray</surname><given-names>R</given-names></name><name><surname>Mountford</surname><given-names>CE</given-names></name><name><surname>Sorrell</surname><given-names>TC</given-names></name></person-group><article-title>Rapid identification of candida species by using nuclear magnetic resonance spectroscopy and a statistical classification strategy</article-title><source>Appl Environm Microbiol</source><year>2003</year><volume>69</volume><fpage>4566</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1128/AEM.69.8.4566-4574.2003</pub-id></citation></ref><ref id="B40"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mevik</surname><given-names>B-H</given-names></name><name><surname>Wehrens</surname><given-names>R</given-names></name></person-group><article-title>The pls Package: Principal Component and Partial Least Squares Regression in R</article-title><source>J Stat Software</source><year>2007</year><volume>18</volume><fpage>1</fpage><lpage>24</lpage></citation></ref><ref id="B41"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Liaw</surname><given-names>A</given-names></name><name><surname>Wiener</surname><given-names>M</given-names></name></person-group><article-title>Classification and Regression by randomForest</article-title><source>R News</source><year>2002</year><volume>2</volume><fpage>18</fpage><lpage>22</lpage></citation></ref><ref id="B42"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hothorn</surname><given-names>T</given-names></name><name><surname>Leisch</surname><given-names>F</given-names></name><name><surname>Zeileis</surname><given-names>A</given-names></name><name><surname>Hornik</surname><given-names>K</given-names></name></person-group><article-title>The design and analysis of benchmark experiments</article-title><source>J Comp Graph Stat</source><year>2005</year><volume>14</volume><fpage>575</fpage><lpage>699</lpage></citation></ref><ref id="B43"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Demsar</surname><given-names>J</given-names></name></person-group><article-title>Statistical comparisons of classifiers over multiple data sets</article-title><source>J Mach Learn Res</source><year>2006</year><volume>7</volume><fpage>1</fpage><lpage>30</lpage></citation></ref></ref-list></back></article>


